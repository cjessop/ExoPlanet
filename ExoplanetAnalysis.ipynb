{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chris\\OneDrive\\Documents\\ExoPlanetDataAnalysis\\ExoplanetAnalysis.ipynb Cell 1\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chris/OneDrive/Documents/ExoPlanetDataAnalysis/ExoplanetAnalysis.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chris/OneDrive/Documents/ExoPlanetDataAnalysis/ExoplanetAnalysis.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mmatplotlib\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minline\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/chris/OneDrive/Documents/ExoPlanetDataAnalysis/ExoplanetAnalysis.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chris/OneDrive/Documents/ExoPlanetDataAnalysis/ExoplanetAnalysis.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m sns\u001b[39m.\u001b[39mset_theme()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chris/OneDrive/Documents/ExoPlanetDataAnalysis/ExoplanetAnalysis.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import mpl_style\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "import warnings\n",
    "\n",
    "# %pip install numpy==1.20.3\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy import stats\n",
    "\n",
    "#import sklearn packages\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(mpl_style.style1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data  \n",
    "df = pd.read_csv('./exoplanets.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'kepid':'KepID',\n",
    "'kepoi_name':'KOIName',\n",
    "'kepler_name':'KeplerName',\n",
    "'koi_disposition':'ExoplanetArchiveDisposition',\n",
    "'koi_pdisposition':'DispositionUsingKeplerData',\n",
    "'koi_score':'DispositionScore',\n",
    "'koi_fpflag_nt':'NotTransit-LikeFalsePositiveFlag',\n",
    "'koi_fpflag_ss':'koi_fpflag_ss',\n",
    "'koi_fpflag_co':'CentroidOffsetFalsePositiveFlag',\n",
    "'koi_fpflag_ec':'EphemerisMatchIndicatesContaminationFalsePositiveFlag',\n",
    "'koi_period':'OrbitalPeriod[days]',\n",
    "'koi_period_err1':'OrbitalPeriodUpperUnc.[days]',\n",
    "'koi_period_err2':'OrbitalPeriodLowerUnc.[days]',\n",
    "'koi_time0bk':'TransitEpoch[BKJD]',\n",
    "'koi_time0bk_err1':'TransitEpochUpperUnc.[BKJD]',\n",
    "'koi_time0bk_err2':'TransitEpochLowerUnc.[BKJD]',\n",
    "'koi_impact':'ImpactParameter',\n",
    "'koi_impact_err1':'ImpactParameterUpperUnc',\n",
    "'koi_impact_err2':'ImpactParameterLowerUnc',\n",
    "'koi_duration':'TransitDuration[hrs]',\n",
    "'koi_duration_err1':'TransitDurationUpperUnc.[hrs]',\n",
    "'koi_duration_err2':'TransitDurationLowerUnc.[hrs]',\n",
    "'koi_depth':'TransitDepth[ppm]',\n",
    "'koi_depth_err1':'TransitDepthUpperUnc.[ppm]',\n",
    "'koi_depth_err2':'TransitDepthLowerUnc.[ppm]',\n",
    "'koi_prad':'PlanetaryRadius[Earthradii]',\n",
    "'koi_prad_err1':'PlanetaryRadiusUpperUnc.[Earthradii]',\n",
    "'koi_prad_err2':'PlanetaryRadiusLowerUnc.[Earthradii]',\n",
    "'koi_teq':'EquilibriumTemperature[K',\n",
    "'koi_teq_err1':'EquilibriumTemperatureUpperUnc.[K]',\n",
    "'koi_teq_err2':'EquilibriumTemperatureLowerUnc.[K]',\n",
    "'koi_insol':'InsolationFlux[Earthflux]',\n",
    "'koi_insol_err1':'InsolationFluxUpperUnc.[Earthflux]',\n",
    "'koi_insol_err2':'InsolationFluxLowerUnc.[Earthflux]',\n",
    "'koi_model_snr':'TransitSignal-to-Noise',\n",
    "'koi_tce_plnt_num':'TCEPlanetNumber',\n",
    "'koi_tce_delivname':'TCEDeliver',\n",
    "'koi_steff':'StellarEffectiveTemperature[K]',\n",
    "'koi_steff_err1':'StellarEffectiveTemperatureUpperUnc.[K]',\n",
    "'koi_steff_err2':'StellarEffectiveTemperatureLowerUnc.[K]',\n",
    "'koi_slogg':'StellarSurfaceGravity[log10(cm/s**2)]',\n",
    "'koi_slogg_err1':'StellarSurfaceGravityUpperUnc.[log10(cm/s**2)]',\n",
    "'koi_slogg_err2':'StellarSurfaceGravityLowerUnc.[log10(cm/s**2)]',\n",
    "'koi_srad':'StellarRadius[Solarradii]',\n",
    "'koi_srad_err1':'StellarRadiusUpperUnc.[Solarradii]',\n",
    "'koi_srad_err2':'StellarRadiusLowerUnc.[Solarradii]',\n",
    "'ra':'RA[decimaldegrees]',\n",
    "'dec':'Dec[decimaldegrees]',\n",
    "'koi_kepmag':'Kepler-band[mag]'\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ExoPlanetCandidate'] = df['DispositionUsingKeplerData'].apply(lambda x: 1 if x == 'CANDIDATE' else 0)\n",
    "df['ExoPlanetConfirmed'] = df['ExoplanetArchiveDisposition'].apply(lambda x: 2 if x == 'CONFIRMED' else 1 if x == 'CANDIDATE' else 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set_style(\"darkgrid\")\n",
    "# sns.set(rc={'figure.figsize':(8.7,12.27)})\n",
    "# sns.countplot(x='StellarEclipseFalsePositiveFlag', data=df_dropped, palette='muted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped.drop(columns=['KeplerName','EquilibriumTemperatureUpperUnc.[K]','KOIName',\n",
    "                 'KepID','ExoplanetArchiveDisposition','TCEDeliver',\n",
    "                 'NotTransit-LikeFalsePositiveFlag','EphemerisMatchIndicatesContaminationFalsePositiveFlag',\n",
    "                 'koi_fpflag_ss','CentroidOffsetFalsePositiveFlag',\n",
    "                 'DispositionUsingKeplerData',\n",
    "                 'EquilibriumTemperatureLowerUnc.[K]'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped.isna().any()\n",
    "df_dropped.dropna(inplace=True) # Remove all columns with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('minimum right ascension: ' + str(df_dropped['RA[decimaldegrees]'].min()) + ' degrees')\n",
    "print('maximum right ascension: ' + str(df_dropped['RA[decimaldegrees]'].max()) + ' degrees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 2)\n",
    "# ax[0] = plt.hist(df['StellarRadius[Solar radii]'], bins=100)\n",
    "# ax[1] = plt.hist(df['StellarEffectiveTemperature[K]'], bins=100)\n",
    "\n",
    "\n",
    "\n",
    "#plt.hist(df['StellarRadius[Solar radii]'], bins=1000, alpha=0.5)\n",
    "kde = stats.gaussian_kde(df_dropped['StellarEffectiveTemperature[K]'])\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "plt.hist(df_dropped['StellarEffectiveTemperature[K]'], bins=100, color='red', alpha=0.9)\n",
    "xx = np.linspace(0, 10000, 1000)\n",
    "plt.plot(xx, kde(xx), color='black', linewidth=2.5)\n",
    "\n",
    "# plt.ylim(0, 1000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figsize=(40, 40)\n",
    "sns.set_style('darkgrid')\n",
    "#plt.plot(df['RA [decimal degrees]'], df['Dec [decimal degrees]'], 'o', markersize=0.5, alpha=0.3)\n",
    "#plot right ascension and declination for data points that are either confirmed or candidates\n",
    "plt.plot(df_dropped[df_dropped['ExoPlanetCandidate'] == 0]['RA[decimaldegrees]'], df_dropped[df_dropped['ExoPlanetCandidate'] == 0]['Dec[decimaldegrees]'], 'o', markersize=1, alpha=0.3)\n",
    "plt.plot(df_dropped[df_dropped['ExoPlanetCandidate'] == 1]['RA[decimaldegrees]'], df_dropped[df_dropped['ExoPlanetCandidate'] == 1]['Dec[decimaldegrees]'], '+', markersize=3, alpha=0.3)\n",
    "plt.plot(df_dropped[df_dropped['ExoPlanetConfirmed'] == 2]['RA[decimaldegrees]'], df_dropped[df_dropped['ExoPlanetConfirmed'] == 2]['Dec[decimaldegrees]'], '*', markersize=3, alpha=0.3)\n",
    "plt.xlabel('Right Ascension [decimal degrees $^{\\cdot}$]')\n",
    "plt.ylabel('Declination [decimal degrees $^{\\cdot}$]')\n",
    "plt.title('Exoplanet Candidates and Confirmed Exoplanets')\n",
    "plt.legend(['False Flag', 'Candidate', 'Confirmed'], loc='lower right', fontsize='smaller')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove rows and columns that do not have a high correlation with the exoplanet candidate\n",
    "corr = df_dropped.corr()\n",
    "corr = corr[corr['ExoPlanetCandidate'] > 0.1]\n",
    "corr = corr[corr.index]\n",
    "corr = corr.drop(['ExoPlanetCandidate'], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr.head()\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr, annot=False, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_data(df):\n",
    "#     assert isinstance(df, pd.DataFrame), \"df needs to be the correct type\"\n",
    "#     df.dropna(inplace=True)\n",
    "#     indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "\n",
    "\n",
    "#     return df[indices_to_keep].astype(np.float64)\n",
    "\n",
    "# clean_data(df_dropped)\n",
    "\n",
    "def clean_data(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be the correct type\"\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)\n",
    "\n",
    "clean_data(df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_dropped.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_dropped['StellarEffectiveTemperature[K]'], bins=100, color='blue', kde_kws={\"color\": \"orange\", \"lw\": 3, \"label\": \"KDE\"})\n",
    "plt.title('StellarEffectiveTemperature Distribution')\n",
    "plt.xlabel('StellarEffectiveTemperature [K]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the log scale of the oribital periods in a histogram using seaborn\n",
    "sns.set(rc={'figure.figsize':(8.7,12.27)})\n",
    "sns.histplot(x='OrbitalPeriod[days]', data=df_dropped, log_scale=True, palette=\"bright\", hue='ExoPlanetCandidate', kde=True, bins=100, alpha=0.5)\n",
    "plt.xlim(0, 1500)\n",
    "plt.xlabel('Log Orbital Period [days]')\n",
    "plt.title('Histogram of Orbital Periods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create categorical Plot of the Log Scale Orbital Period versus Number of Planets\n",
    "sns.catplot(x='ExoPlanetCandidate', y='OrbitalPeriod[days]', data=df_dropped, kind='box', palette='muted', showfliers=False)\n",
    "plt.title('Orbital Periods of Exoplanet Candidates')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Log Orbital Period (days)')\n",
    "plt.ylim(0.1, 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature selection to the data\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a function to apply feature selection to the data\n",
    "def feature_selection(df, k):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be the correct type\"\n",
    "    assert isinstance(k, int), \"k needs to be the correct type\" # k is the number of features to select\n",
    "    X = df.drop(['ExoPlanetCandidate'], axis=1)\n",
    "    y = df['ExoPlanetCandidate']\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X = SelectKBest(f_classif, k=k).fit_transform(X, y)\n",
    "    return X, y\n",
    "\n",
    "# Apply feature selection to the data\n",
    "#X, y = feature_selection(df_dropped, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_dropped.drop(['ExoPlanetCandidate', 'ExoPlanetConfirmed'], axis=1)\n",
    "y = df_dropped.ExoPlanetCandidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply test train split to the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, criterion='gini', random_state=21)\n",
    "rf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the accuracy of the model\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print('Accuracy of random forest classifier on test set: {:.2f}'.format(accuracy_score(y_test, y_pred_rf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "# Reset sns style\n",
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "sns.set_style(\"darkgrid\")\n",
    "# Plot the confusion matrix\n",
    "sns.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "#Apply actual labels to the confusion matrix axis\n",
    "tick_marks = np.arange(len(y_test.unique()))\n",
    "plt.xticks(tick_marks, y_test.unique())\n",
    "plt.yticks(tick_marks, y_test.unique())\n",
    "\n",
    "# Add a title\n",
    "plt.title('Confusion Matrix for Random Forest Classifier')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model\n",
    "lr = LogisticRegression(C=100, max_iter=200, class_weight='balanced', solver='liblinear')\n",
    "\n",
    "# Fitting Model to the train set\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Check the accuracy of the model\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "print('Accuracy of linear regression classifier on test set: {:.2f}'.format(accuracy_score(y_test, y_pred_lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improve the model using a CV Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a function to apply a random grid search to the data that runs in a sensible amount of time\n",
    "def grid_search(X, y, model, param_grid, cv):\n",
    "    assert isinstance(X, pd.DataFrame), \"X needs to be the correct type\"\n",
    "    assert isinstance(y, pd.Series), \"y needs to be the correct type\"\n",
    "    assert isinstance(model, object), \"model needs to be the correct type\"\n",
    "    assert isinstance(param_grid, dict), \"param_grid needs to be the correct type\"\n",
    "    assert isinstance(cv, int), \"cv needs to be the correct type\"\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv, n_jobs=-1, verbose=2)\n",
    "    grid.fit(X, y)\n",
    "    return grid.best_score_, grid.best_estimator_\n",
    "\n",
    "#track progress of grid search\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "f1 = make_scorer(f1_score , average='weighted')\n",
    "\n",
    "\n",
    "# Apply a grid search to the data\n",
    "param_grid = {'n_estimators': [100, 200, 300, 400, 500],\n",
    "              #'max_features': ['auto', 'sqrt', 'log2'],\n",
    "              'max_depth': [4, 5, 6, 7, 8],\n",
    "              #'criterion': ['gini', 'entropy']}\n",
    "            }\n",
    "best_score, best_model = grid_search(X_train, y_train, rf, param_grid, 5)\n",
    "print('Best score: ' + str(best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the best model\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "import pickle\n",
    "# filename = 'best_rf_model_binary.sav'\n",
    "# pickle.dump(best_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second round of grid search using the best model from the first round, but now altering max_features and criterion\n",
    "param_grid = {'n_estimators': [200],\n",
    "              'max_features': ['auto', 'sqrt', 'log2'],\n",
    "              'max_depth': [6],\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "best_score2, best_model2 = grid_search(X_train, y_train, best_model, param_grid, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best score for model 2: ' + str(best_score2))\n",
    "#print the best combination of max_features and criterion\n",
    "print(best_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an adaboost classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier(random_state=21)\n",
    "ada.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the accuracy of the model\n",
    "y_pred = ada.predict(X_test)\n",
    "print('Accuracy of AdaBoost classifier on test set: {:.2f}'.format(accuracy_score(y_test, y_pred)))\n",
    "# Worse than random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import os\n",
    "estimator = rf.estimators_[5]\n",
    "# Export as dot file\n",
    "export_graphviz(estimator, out_file='tree.dot', \n",
    "                feature_names = X.columns,\n",
    "                class_names = ['Not Candidate', 'Candidate'],\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('dot -Tpng tree.dot -o tree.png')\n",
    "#from subprocess import call\n",
    "#call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline to apply a grid search to the data\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a pipeline\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('pca', PCA()),\n",
    "                 ('classifier', RandomForestClassifier(random_state=21))])\n",
    "\n",
    "# Create a parameter grid\n",
    "param_grid = {'classifier__n_estimators': [100, 200, 300, 400, 500],\n",
    "              'classifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "              'classifier__max_depth': [4, 5, 6, 7, 8],\n",
    "              'classifier__criterion': ['gini', 'entropy']}\n",
    "# Create a grid search object\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "# Fit the grid search\n",
    "grid.fit(X_train, y_train)\n",
    "# Output the best score\n",
    "print(grid.best_score_)\n",
    "# Output the best estimator\n",
    "print(grid.best_estimator_)\n",
    "# Output the best parameters\n",
    "print(grid.best_params_)\n",
    "# Output the best model\n",
    "best_model = grid.best_estimator_\n",
    "# Check the accuracy of the model\n",
    "y_pred = best_model.predict(X_test)\n",
    "print('Accuracy of best model on test set: {:.2f}'.format(accuracy_score(y_test, y_pred)))\n",
    "# Still worse than the random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of classifiers to compare the accuracy of the models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of classifiers\n",
    "classifiers = [LogisticRegression(solver='liblinear'),\n",
    "               DecisionTreeClassifier(),\n",
    "               KNeighborsClassifier(),\n",
    "               RandomForestClassifier(),\n",
    "               AdaBoostClassifier(),\n",
    "               GradientBoostingClassifier(),\n",
    "               SVC(),\n",
    "               GaussianNB()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to compare the accuracy of the models\n",
    "def compare_accuracy(X, y, classifiers):\n",
    "    assert isinstance(X, pd.DataFrame), \"X needs to be the correct type\"\n",
    "    assert isinstance(y, pd.Series), \"y needs to be the correct type\"\n",
    "    assert isinstance(classifiers, list), \"classifiers needs to be the correct type\"\n",
    "    accuracy = []\n",
    "    for classifier in classifiers:\n",
    "        model = classifier\n",
    "        model.fit(X, y)\n",
    "        scores = cross_val_score(model, X, y, cv=5)\n",
    "        accuracy.append(scores.mean())\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the data\n",
    "accuracy = compare_accuracy(X_train, y_train, classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of the accuracy scores\n",
    "accuracy_df = pd.DataFrame(accuracy, index=['LogisticRegression',\n",
    "                                            'DecisionTreeClassifier',\n",
    "                                            'KNeighborsClassifier',\n",
    "                                            'RandomForestClassifier',\n",
    "                                            'AdaBoostClassifier',\n",
    "                                            'GradientBoostingClassifier',\n",
    "                                            'SVC',\n",
    "                                            'GaussianNB'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the accuracy column\n",
    "accuracy_df.rename(columns={0: 'Accuracy'}, inplace=True)\n",
    "# Sort the dataframe by accuracy\n",
    "accuracy_df.sort_values(by='Accuracy', ascending=False, inplace=True)\n",
    "# Output the dataframe\n",
    "print(accuracy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform last test with a multi layer perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(30,30,30))\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred_mlp = mlp.predict(X_test)\n",
    "print('Accuracy of MLP classifier on test set: {:.2f}'.format(accuracy_score(y_test, y_pred_mlp)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Improve the MLP classifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)\n",
    "# Apply the transformations to the data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(30,30,30))\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred_mlp = mlp.predict(X_test)\n",
    "print('Accuracy of MLP classifier on test set: {:.2f}'.format(accuracy_score(y_test, y_pred_mlp)))\n",
    "# Far better than before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ensemble of the best models\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "# Create a list of the best models\n",
    "models = [('lr', lr), ('rf', rf), ('mlp', mlp)]\n",
    "# Create a voting classifier\n",
    "ensemble = VotingClassifier(estimators=models)\n",
    "# Fit the voting classifier\n",
    "ensemble.fit(X_train, y_train)\n",
    "# Check the accuracy of the model\n",
    "y_pred_ensemble = ensemble.predict(X_test)\n",
    "print('Accuracy of ensemble classifier on test set: {:.2f}'.format(accuracy_score(y_test, y_pred_ensemble)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification report for the ensemble\n",
    "print(classification_report(y_test, y_pred_ensemble))\n",
    "# Recall is the ability of the classifier to find all the positive samples\n",
    "# Precision is the ability of the classifier not to label as positive a sample that is negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix for the ensemble\n",
    "# confusion_matrix_ensemble = confusion_matrix(y_test, y_pred_ensemble)\n",
    "\n",
    "# # Reset sns style\n",
    "# sns.set_style(\"darkgrid\")\n",
    "# # Plot the confusion matrix\n",
    "# sns.heatmap(confusion_matrix_ensemble, annot=True, cmap='Blues', fmt='g')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('True')\n",
    "# #Apply actual labels to the confusion matrix axis\n",
    "# tick_marks = np.arange(len(y_test.unique()))\n",
    "# plt.xticks(tick_marks, y_test.unique())\n",
    "# plt.yticks(tick_marks, y_test.unique())\n",
    "\n",
    "# # Add a title\n",
    "# plt.title('Confusion Matrix for Random Forest Classifier')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create streamlit app\n",
    "import streamlit as st\n",
    "\n",
    "# Create a title\n",
    "#st.title('Exoplanet Classification')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
